{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo Bimbo Inventory Demand\n",
    "[Link](https://www.kaggle.com/c/grupo-bimbo-inventory-demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing \"producto_tabla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------+\n",
      "|product_id|product_name                            |\n",
      "+----------+----------------------------------------+\n",
      "|0         |NO IDENTIFICADO 0                       |\n",
      "|9         |Capuccino Moka 750g NES 9               |\n",
      "|41        |Bimbollos Ext sAjonjoli 6p 480g BIM 41  |\n",
      "|53        |Burritos Sincro 170g CU LON 53          |\n",
      "|72        |Div Tira Mini Doradita 4p 45g TR 72     |\n",
      "|73        |Pan Multigrano Linaza 540g BIM 73       |\n",
      "|98        |Tostado Integral 180g WON 98            |\n",
      "|99        |Pan Blanco 567g WON 99                  |\n",
      "|100       |Super Pan Bco Ajonjoli 680g SP WON 100  |\n",
      "|106       |Wonder 100pct mediano 475g WON 106      |\n",
      "|107       |Wonder 100pct gde 680g SP WON 107       |\n",
      "|108       |Baguette Precocida Cong 280g DH 108     |\n",
      "|109       |Pan Multicereal 475g WON 109            |\n",
      "|112       |Tostado Integral 180g WON 112           |\n",
      "|122       |Biscotel Receta Original 410g CU SUA 122|\n",
      "|123       |Super Bollos 5in 8p 540g WON 123        |\n",
      "|125       |Bollos 8p 450g WON 125                  |\n",
      "|131       |Bollos BK 4in 36p 1635g SL 131          |\n",
      "|132       |Bollos BK 5in 30p 1730g SL 132          |\n",
      "|134       |Bollos BK 4in 30p 1635g TIR SL 134      |\n",
      "+----------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load producto_tabla data\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "path = '/home/jasam/Downloads/bimbo/producto_tabla.csv'\n",
    "product_table_rdd = (sc.textFile(path)\n",
    "                     .map(lambda line: line.split(\",\")))\n",
    "\n",
    "product_table_df = (product_table_rdd\n",
    "                    .toDF(['product_id','product_name']))\n",
    "#drop Header\n",
    "product_table_df = (product_table_df\n",
    "                    .filter(col('product_id') != 'Producto_ID'))\n",
    "\n",
    "product_table_df.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls in product_tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_null(col_name):\n",
    "    return sum(col(col_name).isNull().cast('integer')).alias(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         0|           0|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exprs = []\n",
    "for col_name in product_table_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "product_table_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count rows - producto_tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,592'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(product_table_df.count(), ',d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing \"cliente_tabla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------+\n",
      "|customer_id|customer_name                          |\n",
      "+-----------+---------------------------------------+\n",
      "|0          |SIN NOMBRE                             |\n",
      "|1          |OXXO XINANTECATL                       |\n",
      "|2          |SIN NOMBRE                             |\n",
      "|3          |EL MORENO                              |\n",
      "|4          |SDN SER  DE ALIM  CUERPO SA CIA  DE INT|\n",
      "|4          |SDN SER DE ALIM CUERPO SA CIA DE INT   |\n",
      "|5          |LA VAQUITA                             |\n",
      "|6          |LUPITA                                 |\n",
      "|7          |I M EL GUERO                           |\n",
      "|8          |MINI SUPER LOS LUPES                   |\n",
      "|9          |SUPER KOMPRAS MICRO COLON              |\n",
      "|10         |LONJA MERCANTIL DE TODO                |\n",
      "|11         |FARMACIA NICOLAS SAN JUAN              |\n",
      "|12         |PAPELERIA CATALA                       |\n",
      "|13         |ELENA                                  |\n",
      "|14         |CASA TRINO                             |\n",
      "|15         |FMA035947 BIMBO SA DE CV               |\n",
      "|16         |JOYS                                   |\n",
      "|17         |DE MARCO                               |\n",
      "|18         |LUPES II                               |\n",
      "+-----------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load producto_tabla data\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "path = '/home/jasam/Downloads/bimbo/cliente_tabla.csv'\n",
    "customer_table_rdd = (sc.textFile(path)\n",
    "                      .map(lambda line: line.split(\",\")))\n",
    "\n",
    "customer_table_df = (customer_table_rdd\n",
    "                     .toDF(['customer_id','customer_name']))\n",
    "#drop Header\n",
    "customer_table_df = (customer_table_df\n",
    "                     .filter(col('customer_id') != 'Cliente_ID'))\n",
    "\n",
    "customer_table_df.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls in customer_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|customer_name|\n",
      "+-----------+-------------+\n",
      "|          0|            0|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exprs = []\n",
    "for col_name in customer_table_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "customer_table_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count rows - cliente_tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'935,362'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(customer_table_df.count(), ',d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing \"town_state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+----------------+\n",
      "|agency_id|town                   |state           |\n",
      "+---------+-----------------------+----------------+\n",
      "|1110     |2008 AG. LAGO FILT     |\"MÉXICO D.F.\"   |\n",
      "|1111     |2002 AG. AZCAPOTZALCO  |\"MÉXICO D.F.\"   |\n",
      "|1112     |2004 AG. CUAUTITLAN    |ESTADO DE MÉXICO|\n",
      "|1113     |2008 AG. LAGO FILT     |\"MÉXICO D.F.\"   |\n",
      "|1114     |2029 AG.IZTAPALAPA 2   |\"MÉXICO D.F.\"   |\n",
      "|1116     |2011 AG. SAN ANTONIO   |\"MÉXICO D.F.\"   |\n",
      "|1117     |2001 AG. ATIZAPAN      |ESTADO DE MÉXICO|\n",
      "|1118     |2007 AG. LA VILLA      |\"MÉXICO D.F.\"   |\n",
      "|1119     |2013 AG. MEGA NAUCALPAN|ESTADO DE MÉXICO|\n",
      "|1120     |2018 AG. TEPALCATES 2  |\"MÉXICO D.F.\"   |\n",
      "|1121     |2016 AG. SAN LORENZO   |\"MÉXICO D.F.\"   |\n",
      "|1122     |2019 AG. XALOSTOC      |ESTADO DE MÉXICO|\n",
      "|1123     |2094 CHALCO_BM         |ESTADO DE MÉXICO|\n",
      "|1124     |2021 AG. XOCHIMILCO 2  |\"MÉXICO D.F.\"   |\n",
      "|1126     |2017 AG. SANTA CLARA   |ESTADO DE MÉXICO|\n",
      "|1127     |2003 AG. COACALCO      |ESTADO DE MÉXICO|\n",
      "|1129     |2011 AG. SAN ANTONIO   |\"MÉXICO D.F.\"   |\n",
      "|1130     |2010 AG. LOS REYES     |ESTADO DE MÉXICO|\n",
      "|1137     |2014 AG. NEZA          |ESTADO DE MÉXICO|\n",
      "|1138     |2015 AG. ROJO GOMEZ    |\"MÉXICO D.F.\"   |\n",
      "+---------+-----------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load producto_tabla data\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "path = '/home/jasam/Downloads/bimbo/town_state.csv'\n",
    "\n",
    "# Clean data\n",
    "town_state_table_rdd = (sc.textFile(path)\n",
    "                        .map(lambda line: line.replace('MÉXICO,','MÉXICO')))\n",
    "\n",
    "town_state_table_rdd = (town_state_table_rdd\n",
    "                        .map(lambda line: line.split(\",\")))\n",
    "town_state_table_df = (town_state_table_rdd\n",
    "                       .toDF(['agency_id','town','state']))\n",
    "#drop Header\n",
    "town_state_table_df = (town_state_table_df\n",
    "                       .filter(col('agency_id') != 'Agencia_ID'))\n",
    "\n",
    "town_state_table_df.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls in town_state_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|agency_id|town|state|\n",
      "+---------+----+-----+\n",
      "|        0|   0|    0|\n",
      "+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exprs = []\n",
    "for col_name in town_state_table_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "town_state_table_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count rows - town_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'790'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(town_state_table_df.count(), ',d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by state - count  - town_state table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               state|count|\n",
      "+--------------------+-----+\n",
      "|    ESTADO DE MÉXICO|   71|\n",
      "|       \"MÉXICO D.F.\"|   65|\n",
      "|             JALISCO|   55|\n",
      "|            VERACRUZ|   45|\n",
      "|          GUANAJUATO|   39|\n",
      "|              SONORA|   34|\n",
      "|          NUEVO LEÓN|   34|\n",
      "|              PUEBLA|   34|\n",
      "|           MICHOACÁN|   33|\n",
      "|          TAMAULIPAS|   32|\n",
      "|BAJA CALIFORNIA N...|   32|\n",
      "|            COAHUILA|   29|\n",
      "|           CHIHUAHUA|   25|\n",
      "|             SINALOA|   23|\n",
      "|              OAXACA|   22|\n",
      "|            GUERRERO|   21|\n",
      "|             HIDALGO|   21|\n",
      "| BAJA CALIFORNIA SUR|   17|\n",
      "|             CHIAPAS|   16|\n",
      "|     SAN LUIS POTOSÍ|   15|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(town_state_table_df.groupBy('state')\n",
    "                    .count()\n",
    "                    .orderBy(col('count').desc())\n",
    "                    .show(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing \"train data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------+--------+----------------+\n",
      "|sale_uni_today|sale_today|dev_uni_prox|dev_prox|demand_uni_equil|\n",
      "+--------------+----------+------------+--------+----------------+\n",
      "|3.0           |25.14     |0.0         |0.0     |3.0             |\n",
      "|4.0           |33.52     |0.0         |0.0     |4.0             |\n",
      "|4.0           |39.32     |0.0         |0.0     |4.0             |\n",
      "|4.0           |33.52     |0.0         |0.0     |4.0             |\n",
      "|3.0           |22.92     |0.0         |0.0     |3.0             |\n",
      "|5.0           |38.2      |0.0         |0.0     |5.0             |\n",
      "|3.0           |20.28     |0.0         |0.0     |3.0             |\n",
      "|6.0           |56.1      |0.0         |0.0     |6.0             |\n",
      "|4.0           |24.6      |0.0         |0.0     |4.0             |\n",
      "|6.0           |31.68     |0.0         |0.0     |6.0             |\n",
      "+--------------+----------+------------+--------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load producto_tabla data\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "path = '/data/train.csv'\n",
    "train_table_rdd = (sc.textFile(path)\n",
    "                   .map(lambda line: line.split(\",\")))\n",
    "columns = ['week','agency_id','channel','route_sak','customer_id',\n",
    "           'product_id','sale_uni_today','sale_today','dev_uni_prox',\n",
    "           'dev_prox','demand_uni_equil']\n",
    "train_table_df = (train_table_rdd\n",
    "                  .toDF(columns))\n",
    "\n",
    "#drop Header\n",
    "train_table_df = (train_table_df\n",
    "                  .filter(col('week') != 'Semana'))\n",
    "#cast data\n",
    "train_table_df = train_table_df.select(train_table_df.sale_uni_today.cast('float'),\n",
    "                                       train_table_df.sale_today.cast('float'),\n",
    "                                       train_table_df.dev_uni_prox.cast('float'),\n",
    "                                       train_table_df.dev_prox.cast('float'),\n",
    "                                       train_table_df.demand_uni_equil.cast('float'))\n",
    "train_table_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_uni_today: float (nullable = true)\n",
      " |-- sale_today: float (nullable = true)\n",
      " |-- dev_uni_prox: float (nullable = true)\n",
      " |-- dev_prox: float (nullable = true)\n",
      " |-- demand_uni_equil: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls in train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------+--------+----------------+\n",
      "|sale_uni_today|sale_today|dev_uni_prox|dev_prox|demand_uni_equil|\n",
      "+--------------+----------+------------+--------+----------------+\n",
      "|             0|         0|           0|       0|               0|\n",
      "+--------------+----------+------------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exprs = []\n",
    "for col_name in train_table_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "train_table_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count rows - train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'74,180,464'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(train_table_df.count(), ',d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics - train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|    sale_uni_today|       sale_today|      dev_uni_prox|          dev_prox|  demand_uni_equil|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|          74180464|         74180464|          74180464|          74180464|          74180464|\n",
      "|   mean| 7.310163468376256|68.54452256746646|0.1302576646056029|1.2432480508547747|7.2245640038056385|\n",
      "| stddev|21.967336840002165|338.9795164568217|29.323204167613063|39.215522512196806| 21.77119294903071|\n",
      "|    min|               0.0|              0.0|               0.0|               0.0|               0.0|\n",
      "|    max|            7200.0|         647360.0|          250000.0|          130760.0|            5000.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_table_df.describe('sale_uni_today', 'sale_today', 'dev_uni_prox', 'dev_prox', 'demand_uni_equil').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics - \"correlation\" between unitary sales  and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7336777290062536"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table_df_cor = (train_table_df.select(train_table_df.sale_uni_today.cast('float'), \n",
    "                                            train_table_df.sale_today.cast('float')))\n",
    "train_table_df_cor.stat.corr('sale_uni_today', 'sale_today')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics - \"correlation\" between dev_uni_prox  and dev_prox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1294211664137144"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table_df_cor = (train_table_df.select(train_table_df.dev_uni_prox.cast('float'), \n",
    "                                            train_table_df.dev_prox.cast('float')))\n",
    "train_table_df_cor.stat.corr('dev_uni_prox', 'dev_prox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot - count by product type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"product_id\" among (sale_uni_today, sale_today, dev_uni_prox, dev_prox, demand_uni_equil);'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jasam/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jasam/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1085.join.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"product_id\" among (sale_uni_today, sale_today, dev_uni_prox, dev_prox, demand_uni_equil);\n\tat org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:152)\n\tat org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:152)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:151)\n\tat org.apache.spark.sql.DataFrame$$anonfun$7.apply(DataFrame.scala:462)\n\tat org.apache.spark.sql.DataFrame$$anonfun$7.apply(DataFrame.scala:460)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.DataFrame.join(DataFrame.scala:460)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-cb5e1fb3a2a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m product_grouped_df = (train_table_df.join(product_table_df, 'product_id')\n\u001b[0m\u001b[0;32m      2\u001b[0m                                     .select(product_table_df.product_name))\n\u001b[0;32m      3\u001b[0m \u001b[0mproduct_grouped_pandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct_grouped_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'product_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mproduct_grouped_pandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct_grouped_pandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jasam/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m                 \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"how should be basestring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jasam/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jasam/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"product_id\" among (sale_uni_today, sale_today, dev_uni_prox, dev_prox, demand_uni_equil);'"
     ]
    }
   ],
   "source": [
    "product_grouped_df = (train_table_df.join(product_table_df, 'product_id')\n",
    "                                    .select(product_table_df.product_name))\n",
    "product_grouped_pandas = product_grouped_df.groupBy('product_name').count().toPandas()\n",
    "product_grouped_pandas = product_grouped_pandas.sort_values(by='count', ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.plt.title('Bar plot count by product type')\n",
    "plot = sns.barplot(x='count', y='product_name', data=product_grouped_pandas.head(25))\n",
    "plot.set(xlabel='Product Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot sale unitary today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sale_uni_grouped_pandas = train_table_df.groupBy('sale_uni_today').count().toPandas()\n",
    "sale_uni_grouped_pandas = sale_uni_grouped_pandas.sort_values(by='count', ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sale_uni_grouped_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sale_uni_grouped_pandas[['sale_uni_today']] = sale_uni_grouped_pandas[['sale_uni_today']].astype(int)\n",
    "sale_uni_grouped_pandas = sale_uni_grouped_pandas.sort_values(by='count', ascending=0)\n",
    "sns.plt.title('Bar plot count by product type')\n",
    "plot = sns.barplot(x='sale_uni_today', y='count', data=sale_uni_grouped_pandas.head(25))\n",
    "plot.set(xlabel='sale unitary today')\n",
    "plot.set(ylabel='Acum sale unitary today')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sale unity today for \"Mantecadas Vainilla 4p 125g BIM 1240\" distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_grouped_pandas = (train_table_df.join(product_table_df, 'product_id')\n",
    "                                        .select(product_table_df.product_name, train_table_df.sale_uni_today)\n",
    "                                        .filter(product_table_df.product_name == 'Mantecadas Vainilla 4p 125g BIM 1240')\n",
    "                                        .toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#product_grouped_pandas\n",
    "product_grouped_pandas.loc[:,('sale_uni_today')] = product_grouped_pandas.sale_uni_today.astype(int)\n",
    "sns.plt.title('Sale unity today for \"Mantecadas Vainilla 4p 125g BIM 1240\" distribution')\n",
    "sns.distplot(product_grouped_pandas.sale_uni_today, hist=False, color=\"b\", kde_kws={\"shade\": True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
